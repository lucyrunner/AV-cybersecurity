{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23cd67a7",
   "metadata": {},
   "source": [
    "# `evaluate.py`\n",
    "\n",
    "This notebook contains the contents of `evaluate.py` exported from the project. Run cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41353ca",
   "metadata": {},
   "source": [
    "**Quick usage:** Save this notebook and run the code cells. These cells can be edited or executed directly.\n",
    "\n",
    "If you prefer to run as a script, download the `.py` file and run it in a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62332d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, os, csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from acc_env import ACCEnv\n",
    "from attacks import FGSMAttack, OIAttack\n",
    "\n",
    "def make_env(brake_profile=True, normalize_obs=True):\n",
    "    def _thunk():\n",
    "        # Evaluation uses a braking scenario to test reactions\n",
    "        return ACCEnv(brake_profile=brake_profile, normalize_obs=normalize_obs)\n",
    "    return _thunk\n",
    "\n",
    "def load_model_and_env(logdir: str):\n",
    "    env = DummyVecEnv([make_env(brake_profile=True, normalize_obs=True)])\n",
    "    env = VecNormalize.load(os.path.join(logdir, 'vecnormalize.pkl'), env)\n",
    "    env.training = False\n",
    "    env.norm_reward = False\n",
    "    model = PPO.load(os.path.join(logdir, 'ppo_acc'))\n",
    "    return model, env\n",
    "\n",
    "def run_episode(model, env, attack=None, eps=0.01, render_traj=False):\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    traj = {k: [] for k in ['Δx','v','a','r']}\n",
    "    total_r = 0.0\n",
    "    collisions = 0\n",
    "    prev_a = 0.0\n",
    "    rmse_accum = 0.0\n",
    "    rmse_count = 0\n",
    "\n",
    "    # Wrap model with attack if requested\n",
    "    atk = None\n",
    "    if attack == 'fgsm':\n",
    "        atk = FGSMAttack(model, epsilon=eps, device='cpu')\n",
    "    elif attack == 'oia':\n",
    "        atk = OIAttack(model, epsilon=eps, device='cpu')\n",
    "\n",
    "    while True:\n",
    "        if atk is None:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs_in = obs\n",
    "        else:\n",
    "            action, obs_in = atk.act(obs)\n",
    "\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        total_r += reward[0] if isinstance(reward, np.ndarray) else reward\n",
    "        traj['Δx'].append(info[0]['Δx'] if isinstance(info, list) else info['Δx'])\n",
    "        traj['v'].append(info[0]['v'] if isinstance(info, list) else info['v'])\n",
    "        traj['a'].append(info[0]['a'] if isinstance(info, list) else info['a'])\n",
    "        traj['r'].append(reward[0] if isinstance(reward, np.ndarray) else reward)\n",
    "\n",
    "        # For stealth metric: RMSE between obs_in and obs (both normalized)\n",
    "        if atk is not None:\n",
    "            diff = (obs_in - obs)\n",
    "            rmse_accum += float((diff**2).mean())\n",
    "            rmse_count += 1\n",
    "\n",
    "        done = bool(term) or bool(trunc)\n",
    "        if term:\n",
    "            collisions = 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Metrics\n",
    "    jerk = np.mean(np.abs(np.diff(traj['a']))) if len(traj['a']) > 1 else 0.0\n",
    "    rmse = np.sqrt(rmse_accum / max(1, rmse_count))\n",
    "    return {\n",
    "        'return': total_r,\n",
    "        'collision': collisions,\n",
    "        'jerk': jerk,\n",
    "        'rmse': rmse,\n",
    "        'traj': traj\n",
    "    }\n",
    "\n",
    "def plot_traj(traj, title, out_png):\n",
    "    t = np.arange(len(traj['Δx']))\n",
    "    plt.figure()\n",
    "    plt.plot(t, traj['Δx'])\n",
    "    plt.xlabel('t (steps)'); plt.ylabel('Δx (m)'); plt.title(title + ' — headway (Δx)')\n",
    "    plt.savefig(out_png.replace('.png','_dx.png'), bbox_inches='tight'); plt.close()\n",
    "\n",
    "    t = np.arange(len(traj['v']))\n",
    "    plt.figure()\n",
    "    plt.plot(t, traj['v'])\n",
    "    plt.xlabel('t (steps)'); plt.ylabel('v (m/s)'); plt.title(title + ' — ego speed (v)')\n",
    "    plt.savefig(out_png.replace('.png','_v.png'), bbox_inches='tight'); plt.close()\n",
    "\n",
    "    t = np.arange(len(traj['a']))\n",
    "    plt.figure()\n",
    "    plt.plot(t, traj['a'])\n",
    "    plt.xlabel('t (steps)'); plt.ylabel('a (m/s^2)'); plt.title(title + ' — acceleration (a)')\n",
    "    plt.savefig(out_png.replace('.png','_a.png'), bbox_inches='tight'); plt.close()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--logdir', type=str, default='runs/ppo_baseline')\n",
    "    parser.add_argument('--episodes', type=int, default=20)\n",
    "    parser.add_argument('--attack', type=str, default='none', choices=['none','fgsm','oia'])\n",
    "    parser.add_argument('--eps', type=float, default=0.01)\n",
    "    parser.add_argument('--compare', action='store_true', help='Run baseline vs FGSM vs OIA')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "    model, env = load_model_and_env(args.logdir)\n",
    "\n",
    "    def eval_many(which: str | None):\n",
    "        atk = 'none' if which is None else which\n",
    "        rets, cols, jerks, rmses = [], [], [], []\n",
    "        sample_traj = None\n",
    "        for ep in range(args.episodes):\n",
    "            res = run_episode(model, env, attack=which, eps=args.eps)\n",
    "            rets.append(res['return'])\n",
    "            cols.append(res['collision'])\n",
    "            jerks.append(res['jerk'])\n",
    "            rmses.append(res['rmse'])\n",
    "            if sample_traj is None:\n",
    "                sample_traj = res['traj']\n",
    "        avg = {\n",
    "            'avg_return': float(np.mean(rets)),\n",
    "            'collision_rate': float(np.mean(cols)),\n",
    "            'avg_jerk': float(np.mean(jerks)),\n",
    "            'avg_rmse': float(np.mean(rmses)),\n",
    "        }\n",
    "        # Save CSV\n",
    "        out_csv = f\"artifacts/metrics_{atk}.csv\"\n",
    "        with open(out_csv, 'w', newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(['metric','value'])\n",
    "            for k,v in avg.items():\n",
    "                w.writerow([k,v])\n",
    "        # Plots for sample traj\n",
    "        plot_traj(sample_traj, f'{atk.upper()} sample episode', f'artifacts/{atk}_traj.png')\n",
    "        print(f\"{atk}: {avg}\")\n",
    "        return avg\n",
    "\n",
    "    if args.compare:\n",
    "        base = eval_many(None)\n",
    "        fgsm = eval_many('fgsm')\n",
    "        oia  = eval_many('oia')\n",
    "        # Simple print summary\n",
    "        print('--- Comparison ---')\n",
    "        print('Baseline:', base)\n",
    "        print('FGSM    :', fgsm)\n",
    "        print('OIA     :', oia)\n",
    "    else:\n",
    "        which = None if args.attack=='none' else args.attack\n",
    "        eval_many(which)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
