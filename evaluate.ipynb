{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62332d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 20:44:57.403929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'acc_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv, VecNormalize\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01macc_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACCEnv\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mattacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FGSMAttack, OIAttack\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_env\u001b[39m(brake_profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize_obs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'acc_env'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import argparse, os, csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from acc_env import ACCEnv\n",
    "from attacks import FGSMAttack, OIAttack\n",
    "\n",
    "def make_env(brake_profile=True, normalize_obs=True):\n",
    "    def _thunk():\n",
    "        # Evaluation uses a braking scenario to test reactions\n",
    "        return ACCEnv(brake_profile=brake_profile, normalize_obs=normalize_obs)\n",
    "    return _thunk\n",
    "\n",
    "def load_model_and_env(logdir: str):\n",
    "    env = DummyVecEnv([make_env(brake_profile=True, normalize_obs=True)])\n",
    "    env = VecNormalize.load(os.path.join(logdir, 'vecnormalize.pkl'), env)\n",
    "    env.training = False\n",
    "    env.norm_reward = False\n",
    "    model = PPO.load(os.path.join(logdir, 'ppo_acc'))\n",
    "    return model, env\n",
    "\n",
    "def run_episode(model, env, attack=None, eps=0.01, render_traj=False):\n",
    "    obs = env.reset()[0]\n",
    "    done = False\n",
    "    traj = {k: [] for k in ['Δx','v','a','r']}\n",
    "    total_r = 0.0\n",
    "    collisions = 0\n",
    "    prev_a = 0.0\n",
    "    rmse_accum = 0.0\n",
    "    rmse_count = 0\n",
    "\n",
    "    # Wrap model with attack if requested\n",
    "    atk = None\n",
    "    if attack == 'fgsm':\n",
    "        atk = FGSMAttack(model, epsilon=eps, device='cpu')\n",
    "    elif attack == 'oia':\n",
    "        atk = OIAttack(model, epsilon=eps, device='cpu')\n",
    "\n",
    "    while True:\n",
    "        if atk is None:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs_in = obs\n",
    "        else:\n",
    "            action, obs_in = atk.act(obs)\n",
    "\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        total_r += reward[0] if isinstance(reward, np.ndarray) else reward\n",
    "        traj['Δx'].append(info[0]['Δx'] if isinstance(info, list) else info['Δx'])\n",
    "        traj['v'].append(info[0]['v'] if isinstance(info, list) else info['v'])\n",
    "        traj['a'].append(info[0]['a'] if isinstance(info, list) else info['a'])\n",
    "        traj['r'].append(reward[0] if isinstance(reward, np.ndarray) else reward)\n",
    "\n",
    "        # For stealth metric: RMSE between obs_in and obs (both normalized)\n",
    "        if atk is not None:\n",
    "            diff = (obs_in - obs)\n",
    "            rmse_accum += float((diff**2).mean())\n",
    "            rmse_count += 1\n",
    "\n",
    "        done = bool(term) or bool(trunc)\n",
    "        if term:\n",
    "            collisions = 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Metrics\n",
    "    jerk = np.mean(np.abs(np.diff(traj['a']))) if len(traj['a']) > 1 else 0.0\n",
    "    rmse = np.sqrt(rmse_accum / max(1, rmse_count))\n",
    "    return {\n",
    "        'return': total_r,\n",
    "        'collision': collisions,\n",
    "        'jerk': jerk,\n",
    "        'rmse': rmse,\n",
    "        'traj': traj\n",
    "    }\n",
    "\n",
    "def plot_traj(traj, title, out_png):\n",
    "    t = np.arange(len(traj['Δx']))\n",
    "    plt.figure()\n",
    "    plt.plot(t, traj['Δx'])\n",
    "    plt.xlabel('t (steps)'); plt.ylabel('Δx (m)'); plt.title(title + ' — headway (Δx)')\n",
    "    plt.savefig(out_png.replace('.png','_dx.png'), bbox_inches='tight'); plt.close()\n",
    "\n",
    "    t = np.arange(len(traj['v']))\n",
    "    plt.figure()\n",
    "    plt.plot(t, traj['v'])\n",
    "    plt.xlabel('t (steps)'); plt.ylabel('v (m/s)'); plt.title(title + ' — ego speed (v)')\n",
    "    plt.savefig(out_png.replace('.png','_v.png'), bbox_inches='tight'); plt.close()\n",
    "\n",
    "    t = np.arange(len(traj['a']))\n",
    "    plt.figure()\n",
    "    plt.plot(t, traj['a'])\n",
    "    plt.xlabel('t (steps)'); plt.ylabel('a (m/s^2)'); plt.title(title + ' — acceleration (a)')\n",
    "    plt.savefig(out_png.replace('.png','_a.png'), bbox_inches='tight'); plt.close()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--logdir', type=str, default='runs/ppo_baseline')\n",
    "    parser.add_argument('--episodes', type=int, default=20)\n",
    "    parser.add_argument('--attack', type=str, default='none', choices=['none','fgsm','oia'])\n",
    "    parser.add_argument('--eps', type=float, default=0.01)\n",
    "    parser.add_argument('--compare', action='store_true', help='Run baseline vs FGSM vs OIA')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "    model, env = load_model_and_env(args.logdir)\n",
    "\n",
    "    def eval_many(which: str | None):\n",
    "        atk = 'none' if which is None else which\n",
    "        rets, cols, jerks, rmses = [], [], [], []\n",
    "        sample_traj = None\n",
    "        for ep in range(args.episodes):\n",
    "            res = run_episode(model, env, attack=which, eps=args.eps)\n",
    "            rets.append(res['return'])\n",
    "            cols.append(res['collision'])\n",
    "            jerks.append(res['jerk'])\n",
    "            rmses.append(res['rmse'])\n",
    "            if sample_traj is None:\n",
    "                sample_traj = res['traj']\n",
    "        avg = {\n",
    "            'avg_return': float(np.mean(rets)),\n",
    "            'collision_rate': float(np.mean(cols)),\n",
    "            'avg_jerk': float(np.mean(jerks)),\n",
    "            'avg_rmse': float(np.mean(rmses)),\n",
    "        }\n",
    "        # Save CSV\n",
    "        out_csv = f\"artifacts/metrics_{atk}.csv\"\n",
    "        with open(out_csv, 'w', newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(['metric','value'])\n",
    "            for k,v in avg.items():\n",
    "                w.writerow([k,v])\n",
    "        # Plots for sample traj\n",
    "        plot_traj(sample_traj, f'{atk.upper()} sample episode', f'artifacts/{atk}_traj.png')\n",
    "        print(f\"{atk}: {avg}\")\n",
    "        return avg\n",
    "\n",
    "    if args.compare:\n",
    "        base = eval_many(None)\n",
    "        fgsm = eval_many('fgsm')\n",
    "        oia  = eval_many('oia')\n",
    "        # Simple print summary\n",
    "        print('--- Comparison ---')\n",
    "        print('Baseline:', base)\n",
    "        print('FGSM    :', fgsm)\n",
    "        print('OIA     :', oia)\n",
    "    else:\n",
    "        which = None if args.attack=='none' else args.attack\n",
    "        eval_many(which)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6927a4a3-5a80-40fd-8acb-f7c0851b7eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# after training and saving to runs/ppo_baseline/\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model_and_env, eval_many\n\u001b[1;32m      3\u001b[0m model, env \u001b[38;5;241m=\u001b[39m load_model_and_env(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/ppo_baseline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m base_metrics \u001b[38;5;241m=\u001b[39m eval_many(model, env, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "# after training and saving to runs/ppo_baseline/\n",
    "from evaluate import load_model_and_env, eval_many\n",
    "model, env = load_model_and_env(\"runs/ppo_baseline\")\n",
    "base_metrics = eval_many(model, env, which=None, episodes=20, eps=0.0)\n",
    "print(base_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
