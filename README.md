{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3868d1",
   "metadata": {},
   "source": [
    "# `README.md`\n",
    "\n",
    "This notebook contains the contents of `README.md` exported from the project. Run cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d29fd6",
   "metadata": {},
   "source": [
    "# PPO + Adversarial Attacks (FGSM & OIA) in 1D ACC Environment\n",
    "\n",
    "This repo provides a **minimal, reproducible** codebase to train a PPO agent for a simplified Adaptive Cruise Control (ACC) task and evaluate two adversarial attacks:\n",
    "**FGSM** (Fast Gradient Sign Method) and **OIA** (Optimism Induction Attack).\n",
    "\n",
    "It follows the environment, reward, safety filter (CBF), and evaluation ideas summarized in the attached notes.\n",
    "\n",
    " Environment Highlights\n",
    "- 1D car-following: ego (agent) follows a lead vehicle on a straight road.\n",
    "- State: `[Δx, Δv, v]` where `Δx = x_lead - x_ego`, `Δv = v_lead - v_ego`, `v = v_ego`.\n",
    "- Action: ego acceleration `a` (continuous, clipped to [a_min, a_max]).\n",
    "- Reward: speed tracking + safe distance + action penalty.\n",
    "- **Safety filter (CBF)**: clamps unsafe accelerations using a headway-based barrier.\n",
    "\n",
    " Attacks\n",
    "- **FGSM**: Perturbs observation in the direction increasing the policy mean action.\n",
    "- **OIA**: Perturbs observation to *increase the value function* (induces optimism).\n",
    "\n",
    " Quickstart\n",
    "\n",
    "```bash\n",
    "# 1) Create and activate a Python 3.10+ virtual environment\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate           # on Windows: .venv\\Scripts\\activate\n",
    "\n",
    "# 2) Install requirements\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 3) (Optional) Train\n",
    "python train.py --total-steps 300000 --logdir runs/ppo_baseline\n",
    "\n",
    "# 4) Evaluate baseline and attacks (will load last model unless specified)\n",
    "python evaluate.py --episodes 20 --attack none      # baseline\n",
    "python evaluate.py --episodes 20 --attack fgsm\n",
    "python evaluate.py --episodes 20 --attack oia\n",
    "\n",
    "# 5) Produce a comparison run (baseline vs FGSM vs OIA)\n",
    "python evaluate.py --episodes 50 --compare\n",
    "```\n",
    "\n",
    "All plots and CSV metrics are saved to `artifacts/`.\n",
    "\n",
    " Files\n",
    "- `acc_env.py`: Gymnasium environment + safety filter + normalization helpers.\n",
    "- `attacks.py`: FGSM & OIA wrappers for SB3 policies (PyTorch autograd).\n",
    "- `train.py`: PPO training with Stable-Baselines3 + VecNormalize.\n",
    "- `evaluate.py`: Evaluation, metrics (collision rate, return, jerk, RMSE), plots.\n",
    "- `requirements.txt`: minimal dependencies.\n",
    "\n",
    "> Note: You may adjust reward weights and safety parameters in `acc_env.py`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
