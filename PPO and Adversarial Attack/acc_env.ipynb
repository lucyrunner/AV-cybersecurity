{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79820556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ACCEnv(gym.Env):\n",
    "    def __init__(self, dt=0.1, max_steps=500):\n",
    "        super(ACCEnv, self).__init__()\n",
    "        \n",
    "        # Parameters from the paper\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Vehicle parameters\n",
    "        self.ego_pos = 0.0\n",
    "        self.ego_vel = 5.0  # Initial speed as in paper\n",
    "        self.lead_vel = 15.0  # Constant lead speed during training\n",
    "        self.lead_pos = 20.0  # Initial lead position\n",
    "        \n",
    "        # Acceleration limits\n",
    "        self.max_accel = 2.0\n",
    "        self.max_decel = -3.5\n",
    "        \n",
    "        # Safety parameters\n",
    "        self.T_h = 1.5  # Time headway\n",
    "        self.d0 = 5.0   # Standstill distance\n",
    "        \n",
    "        # Target speed\n",
    "        self.v_ref = 15.0\n",
    "        self.v_min = 10.0\n",
    "        self.v_max = 20.0\n",
    "        \n",
    "        # Action space: continuous acceleration\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([self.max_decel], dtype=np.float32), \n",
    "            high=np.array([self.max_accel], dtype=np.float32), \n",
    "            shape=(1,), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # State space: [relative_distance, relative_velocity, ego_velocity, iteration_count]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, -10, 0, 0], dtype=np.float32), \n",
    "            high=np.array([100, 10, 30, max_steps], dtype=np.float32), \n",
    "            shape=(4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # For testing scenarios\n",
    "        self.test_mode = False\n",
    "        self.brake_start_time = None\n",
    "        self.brake_duration = 3.0\n",
    "        self.lead_decel = -2.0\n",
    "        \n",
    "        # For rendering\n",
    "        self.fig = None\n",
    "        self.axs = None\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.ego_pos = 0.0\n",
    "        self.ego_vel = 5.0  # Non-zero initial speed as in paper\n",
    "        \n",
    "        # Randomize lead vehicle initial conditions for diversity\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.lead_pos = np.random.uniform(15, 25)\n",
    "        self.lead_vel = np.random.uniform(12, 18)\n",
    "        \n",
    "        if self.test_mode:\n",
    "            self.lead_vel = 15.0  # Fixed for consistent testing\n",
    "            self.lead_pos = 20.0\n",
    "            self.brake_start_time = 100  # Start braking at step 100\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Get normalized observation as described in paper\"\"\"\n",
    "        rel_dist = self.lead_pos - self.ego_pos\n",
    "        rel_vel = self.lead_vel - self.ego_vel\n",
    "        \n",
    "        # Normalize state to [0, 1] range for stable training\n",
    "        norm_rel_dist = rel_dist / 100.0\n",
    "        norm_rel_vel = (rel_vel + 10) / 20.0  # Map [-10, 10] to [0, 1]\n",
    "        norm_ego_vel = self.ego_vel / 30.0\n",
    "        norm_step = self.current_step / self.max_steps\n",
    "        \n",
    "        return np.array([norm_rel_dist, norm_rel_vel, norm_ego_vel, norm_step], dtype=np.float32)\n",
    "    \n",
    "    def _get_denormalized_state(self):\n",
    "        \"\"\"Get actual physical state values\"\"\"\n",
    "        rel_dist = self.lead_pos - self.ego_pos\n",
    "        rel_vel = self.lead_vel - self.ego_vel\n",
    "        return np.array([rel_dist, rel_vel, self.ego_vel, self.current_step])\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.max_decel, self.max_accel)[0]\n",
    "        \n",
    "        # Apply safety filter (CBF)\n",
    "        safe_action = self._safety_filter(action)\n",
    "        \n",
    "        # Update ego vehicle\n",
    "        self.ego_vel += safe_action * self.dt\n",
    "        self.ego_vel = np.clip(self.ego_vel, 0, 30)  # Physical limits\n",
    "        self.ego_pos += self.ego_vel * self.dt\n",
    "        \n",
    "        # Update lead vehicle\n",
    "        if self.test_mode and self.brake_start_time is not None:\n",
    "            if self.current_step >= self.brake_start_time and \\\n",
    "               self.current_step < self.brake_start_time + self.brake_duration/self.dt:\n",
    "                self.lead_vel = max(0, self.lead_vel + self.lead_decel * self.dt)\n",
    "            elif self.current_step >= self.brake_start_time + self.brake_duration/self.dt:\n",
    "                self.lead_vel = max(0, 15.0 + self.lead_decel * self.brake_duration)  # New speed\n",
    "        \n",
    "        self.lead_pos += self.lead_vel * self.dt\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(safe_action)\n",
    "        \n",
    "        # Check termination conditions\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        collision = (self.lead_pos - self.ego_pos) <= 0\n",
    "        timeout = self.current_step >= self.max_steps\n",
    "        reached_end = self.ego_pos >= 1000  # Arbitrary road length\n",
    "        \n",
    "        if collision:\n",
    "            reward -= 50  # Large penalty for collision\n",
    "            terminated = True\n",
    "        elif reached_end:\n",
    "            reward += 50  # Large reward for completing episode\n",
    "            terminated = True\n",
    "        elif timeout:\n",
    "            truncated = True\n",
    "            \n",
    "        self.current_step += 1\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, {\n",
    "            'collision': collision,\n",
    "            'safe_action': safe_action,\n",
    "            'original_action': action\n",
    "        }\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        \"\"\"Calculate reward according to paper's formulation\"\"\"\n",
    "        rel_dist = self.lead_pos - self.ego_pos\n",
    "        \n",
    "        # Step penalty (encourage faster completion)\n",
    "        r_step = -0.05\n",
    "        \n",
    "        # Speed reward (piecewise linear as in paper)\n",
    "        if self.ego_vel < self.v_min:\n",
    "            r_speed = -0.1 * (self.v_min - self.ego_vel)\n",
    "        elif self.ego_vel > self.v_max:\n",
    "            r_speed = -0.1 * (self.ego_vel - self.v_max)\n",
    "        else:\n",
    "            r_speed = 0.1 * (self.ego_vel - self.v_min)\n",
    "        \n",
    "        # Safety distance penalty\n",
    "        safe_dist = self.d0 + self.T_h * self.ego_vel\n",
    "        if rel_dist < safe_dist:\n",
    "            r_safe = -2.0 * (safe_dist - rel_dist) ** 2\n",
    "        else:\n",
    "            r_safe = 0.0\n",
    "            \n",
    "        # Action penalty (for comfort)\n",
    "        r_action = -0.01 * (action ** 2)\n",
    "        \n",
    "        # Idling penalty (if standing still for too long)\n",
    "        r_idling = -20.0 if self.ego_vel < 0.1 and self.current_step > 10 else 0.0\n",
    "        \n",
    "        total_reward = r_step + r_speed + r_safe + r_action + r_idling\n",
    "        return total_reward\n",
    "    \n",
    "    def _safety_filter(self, action):\n",
    "        \"\"\"CBF safety filter implementation\"\"\"\n",
    "        rel_dist = self.lead_pos - self.ego_pos\n",
    "        rel_vel = self.lead_vel - self.ego_vel\n",
    "        \n",
    "        # Calculate maximum safe acceleration using CBF constraint\n",
    "        # Simplified version assuming lead acceleration = 0\n",
    "        h = rel_dist - self.T_h * self.ego_vel\n",
    "        \n",
    "        if h < 0:  # Already unsafe, emergency braking\n",
    "            return self.max_decel\n",
    "            \n",
    "        # Calculate maximum allowed acceleration\n",
    "        max_safe_accel = (h + rel_vel * self.dt) / (self.T_h * self.dt)\n",
    "        \n",
    "        # Apply constraint\n",
    "        safe_action = min(action, max_safe_accel)\n",
    "        return np.clip(safe_action, self.max_decel, self.max_accel)\n",
    "    \n",
    "    def set_test_mode(self, enabled=True):\n",
    "        self.test_mode = enabled\n",
    "        \n",
    "    def render(self):\n",
    "        if self.fig is None:\n",
    "            self.fig, self.axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "            plt.ion()\n",
    "            \n",
    "        rel_dist = self.lead_pos - self.ego_pos\n",
    "        \n",
    "        self.axs[0,0].clear()\n",
    "        self.axs[0,0].plot(self.ego_pos, 0, 'bo', markersize=10, label='Ego')\n",
    "        self.axs[0,0].plot(self.lead_pos, 0, 'ro', markersize=10, label='Lead')\n",
    "        self.axs[0,0].set_xlim(max(0, self.ego_pos-10), self.lead_pos+10)\n",
    "        self.axs[0,0].set_title('Vehicle Positions')\n",
    "        self.axs[0,0].legend()\n",
    "        \n",
    "        self.axs[0,1].clear()\n",
    "        self.axs[0,1].plot(self.current_step, self.ego_vel, 'bo', label='Ego')\n",
    "        self.axs[0,1].plot(self.current_step, self.lead_vel, 'ro', label='Lead')\n",
    "        self.axs[0,1].set_title('Velocities')\n",
    "        self.axs[0,1].set_ylabel('Speed (m/s)')\n",
    "        self.axs[0,1].legend()\n",
    "        \n",
    "        self.axs[1,0].clear()\n",
    "        self.axs[1,0].plot(self.current_step, rel_dist, 'go', label='Distance')\n",
    "        safe_dist = self.d0 + self.T_h * self.ego_vel\n",
    "        self.axs[1,0].axhline(y=safe_dist, color='r', linestyle='--', label='Safe Distance')\n",
    "        self.axs[1,0].set_title('Relative Distance')\n",
    "        self.axs[1,0].set_ylabel('Distance (m)')\n",
    "        self.axs[1,0].legend()\n",
    "        \n",
    "        self.axs[1,1].clear()\n",
    "        self.axs[1,1].plot(self.current_step, self.ego_vel - self.lead_vel, 'purple', label='Rel Velocity')\n",
    "        self.axs[1,1].set_title('Relative Velocity')\n",
    "        self.axs[1,1].set_ylabel('Velocity Diff (m/s)')\n",
    "        self.axs[1,1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.01)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def close(self):\n",
    "        if self.fig is not None:\n",
    "            plt.close(self.fig)\n",
    "            self.fig = None\n",
    "            self.axs = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
