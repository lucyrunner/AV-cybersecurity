{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba648dce-68d7-470e-9612-7b476656efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, sys, subprocess, shlex\n",
    "from IPython import get_ipython\n",
    "\n",
    "# ---------- 1) Ensure ACCEnv is importable ----------\n",
    "NOTEBOOK = \"acc_env.ipynb\"\n",
    "MODULE = \"acc_env\"\n",
    "\n",
    "try:\n",
    "    # prefer acc_env.py if present\n",
    "    from acc_env import ACCEnv  # type: ignore\n",
    "    print(\"Imported ACCEnv from acc_env.py\")\n",
    "except Exception:\n",
    "    if os.path.exists(NOTEBOOK):\n",
    "        print(\"Converting acc_env.ipynb -> acc_env.py ...\")\n",
    "        subprocess.run(shlex.split(f\"jupyter nbconvert --to python {NOTEBOOK}\"), check=True)\n",
    "        if os.getcwd() not in sys.path:\n",
    "            sys.path.append(os.getcwd())\n",
    "        from acc_env import ACCEnv  # type: ignore\n",
    "        print(\"Imported ACCEnv from converted acc_env.py\")\n",
    "    else:\n",
    "        # fallback: run the notebook directly\n",
    "        print(\"No acc_env.py found; running acc_env.ipynb in this kernel ...\")\n",
    "        get_ipython().run_line_magic(\"run\", f\"./{NOTEBOOK}\")\n",
    "        # ACCEnv should now be present in globals()\n",
    "        if \"ACCEnv\" in globals():\n",
    "            print(\"ACCEnv defined by running acc_env.ipynb\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Could not find or define ACCEnv. Put acc_env.py or acc_env.ipynb in the current folder.\")\n",
    "\n",
    "# ---------- 2) imports for training ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ---------- 3) hyperparams & paths (edit as needed) ----------\n",
    "LOGDIR = \"runs/ppo_baseline\"\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "TOTAL_TIMESTEPS = 200_000   # increase for better performance\n",
    "SEED = 42\n",
    "N_ENVS = 1\n",
    "\n",
    "PPO_PARAMS = dict(\n",
    "    policy = \"MlpPolicy\",\n",
    "    verbose = 1,\n",
    "    seed = SEED,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 3e-4,\n",
    "    gamma = 0.99,\n",
    "    gae_lambda = 0.95,\n",
    "    clip_range = 0.2,\n",
    "    ent_coef = 0.0,\n",
    ")\n",
    "\n",
    "# ---------- 4) env factory ----------\n",
    "def make_env(seed=0, brake_profile=True, normalize_obs=True):\n",
    "    def _thunk():\n",
    "        return ACCEnv(brake_profile=brake_profile, normalize_obs=normalize_obs, seed=seed)\n",
    "    return _thunk\n",
    "\n",
    "# Create vectorized normalized training env\n",
    "base_env = DummyVecEnv([make_env(seed=SEED, brake_profile=False, normalize_obs=True) for _ in range(N_ENVS)])\n",
    "train_env = VecNormalize(base_env, norm_obs=True, norm_reward=True, clip_obs=1.0)\n",
    "\n",
    "print(\"Train env created. obs_space:\", train_env.observation_space, \"act_space:\", train_env.action_space)\n",
    "\n",
    "# ---------- 5) create model, set logger, train ----------\n",
    "model = PPO(**PPO_PARAMS, env=train_env)\n",
    "model.set_logger(configure(LOGDIR, [\"stdout\", \"csv\", \"tensorboard\"]))\n",
    "\n",
    "print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps ...\")\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# ---------- 6) save model + vecnormalize ----------\n",
    "model_path = os.path.join(LOGDIR, \"ppo_acc\")\n",
    "vec_path = os.path.join(LOGDIR, \"vecnormalize.pkl\")\n",
    "model.save(model_path)        # writes ppo_acc.zip\n",
    "train_env.save(vec_path)      # writes vecnormalize.pkl\n",
    "print(\"Saved model ->\", model_path + \".zip\")\n",
    "print(\"Saved VecNormalize ->\", vec_path)\n",
    "\n",
    "# ---------- 7) quick smoke-check using saved artifacts ----------\n",
    "# Build an evaluation env that uses the saved normalization stats\n",
    "eval_base = DummyVecEnv([make_env(seed=123, brake_profile=True, normalize_obs=True)])\n",
    "eval_env = VecNormalize.load(vec_path, eval_base)\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# load saved model (this returns a model that will act on eval_env)\n",
    "model_loaded = PPO.load(model_path + \".zip\", env=eval_env)\n",
    "print(\"Loaded model and vecnormalize for evaluation.\")\n",
    "\n",
    "def run_eval_episode(mdl, env, max_steps=1000):\n",
    "    obs, _ = env.reset()\n",
    "    total_r = 0.0\n",
    "    collided = False\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action, _ = mdl.predict(obs, deterministic=True)\n",
    "        out = env.step(action)\n",
    "        if len(out) == 5:\n",
    "            obs, r, term, trunc, info = out\n",
    "        else:\n",
    "            obs, r, done, info = out\n",
    "            term = done; trunc = False\n",
    "        total_r += float(r[0]) if hasattr(r, \"__len__\") else float(r)\n",
    "        steps += 1\n",
    "        if isinstance(info, dict) and info.get(\"collision\", False):\n",
    "            collided = True\n",
    "        if term or trunc or (steps >= max_steps):\n",
    "            break\n",
    "    return total_r, collided, steps\n",
    "\n",
    "print(\"Running 3 smoke-check episodes ...\")\n",
    "for i in range(3):\n",
    "    ret, coll, steps = run_eval_episode(model_loaded, eval_env)\n",
    "    print(f\"Episode {i}: return={ret:.3f}, collision={coll}, steps={steps}\")\n",
    "\n",
    "# Optional: quick plot of training CSV if available\n",
    "csv_path = os.path.join(LOGDIR, \"progress.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'rollout/ep_rew_mean' in df.columns:\n",
    "            plt.figure(figsize=(8,3))\n",
    "            plt.plot(df['rollout/ep_rew_mean'])\n",
    "            plt.title(\"rollout/ep_rew_mean during training\")\n",
    "            plt.xlabel(\"logging step\")\n",
    "            plt.ylabel(\"mean episode reward\")\n",
    "            plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nDone. You should now have:\")\n",
    "print(\" -\", model_path + \".zip\")\n",
    "print(\" -\", vec_path)\n",
    "print(\"These will be loaded by your attacks notebook automatically next time.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5926c-283b-4bfc-9c19-8b0da1df7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust helpers for Gym/SB3 signatures\n",
    "def reset_unwrap(env, **kwargs):\n",
    "    out = env.reset(**kwargs)\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        obs, _info = out\n",
    "        return obs\n",
    "    return out  # obs only\n",
    "\n",
    "def step_unwrap(env, action):\n",
    "    out = env.step(action)\n",
    "    if len(out) == 5:\n",
    "        obs, reward, terminated, truncated, info = out\n",
    "        return obs, reward, bool(terminated), bool(truncated), info\n",
    "    elif len(out) == 4:\n",
    "        obs, reward, done, info = out\n",
    "        return obs, reward, bool(done), False, info\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected env.step() return length: {len(out)}\")\n",
    "\n",
    "# Rebuild eval_env/model_loaded if they don't exist (uses vec_path/model_path you defined earlier)\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "def make_env(seed=123, brake_profile=True, normalize_obs=True):\n",
    "    def _thunk():\n",
    "        return ACCEnv(brake_profile=brake_profile, normalize_obs=normalize_obs, seed=seed)\n",
    "    return _thunk\n",
    "\n",
    "if 'eval_env' not in globals() or 'model_loaded' not in globals():\n",
    "    eval_base = DummyVecEnv([make_env(seed=123)])\n",
    "    eval_env = VecNormalize.load(vec_path, eval_base)\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "    model_loaded = PPO.load(model_path + \".zip\", env=eval_env)\n",
    "\n",
    "# Evaluation loop using the wrappers everywhere\n",
    "def run_eval_episode(mdl, env, max_steps=1000):\n",
    "    obs = reset_unwrap(env)\n",
    "    total_r = 0.0\n",
    "    collided = False\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action, _ = mdl.predict(obs, deterministic=True)\n",
    "        obs, r, term, trunc, info = step_unwrap(env, action)\n",
    "        total_r += float(r[0]) if hasattr(r, \"__len__\") else float(r)\n",
    "        steps += 1\n",
    "        idict = info if isinstance(info, dict) else (info[0] if hasattr(info, \"__len__\") and len(info) else {})\n",
    "        if idict.get(\"collision\", False):\n",
    "            collided = True\n",
    "        if term or trunc or (steps >= max_steps):\n",
    "            break\n",
    "    return total_r, collided, steps\n",
    "\n",
    "print(\"Running 3 smoke-check episodes ...\")\n",
    "for i in range(3):\n",
    "    ret, coll, steps = run_eval_episode(model_loaded, eval_env)\n",
    "    print(f\"Episode {i}: return={ret:.3f}, collision={coll}, steps={steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152da90-4155-46da-8b79-6c3ccc9aa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-cell train + save + smoke-check for ACCEnv + PPO (robust reset/step handling)\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, subprocess, shlex\n",
    "from IPython import get_ipython\n",
    "\n",
    "# 1) Ensure ACCEnv is importable\n",
    "NOTEBOOK = \"acc_env.ipynb\"\n",
    "try:\n",
    "    from acc_env import ACCEnv  # prefer acc_env.py if present\n",
    "    print(\"Imported ACCEnv from acc_env.py\")\n",
    "except Exception:\n",
    "    if os.path.exists(NOTEBOOK):\n",
    "        print(\"Converting acc_env.ipynb -> acc_env.py ...\")\n",
    "        subprocess.run(shlex.split(f\"jupyter nbconvert --to python {NOTEBOOK}\"), check=True)\n",
    "        if os.getcwd() not in sys.path:\n",
    "            sys.path.append(os.getcwd())\n",
    "        from acc_env import ACCEnv\n",
    "        print(\"Imported ACCEnv from converted acc_env.py\")\n",
    "    else:\n",
    "        print(\"No acc_env.py found; running acc_env.ipynb ...\")\n",
    "        get_ipython().run_line_magic(\"run\", f\"./{NOTEBOOK}\")\n",
    "        if \"ACCEnv\" in globals():\n",
    "            print(\"ACCEnv defined by running acc_env.ipynb\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"ACCEnv not found. Put acc_env.py or acc_env.ipynb in this folder.\")\n",
    "\n",
    "# 2) Imports for training\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# 3) Hyperparameters & paths\n",
    "LOGDIR = \"runs/ppo_baseline\"\n",
    "os.makedirs(LOGDIR, exist_ok=True)\n",
    "TOTAL_TIMESTEPS = 200_000\n",
    "SEED = 42\n",
    "N_ENVS = 1\n",
    "\n",
    "PPO_PARAMS = dict(\n",
    "    policy=\"MlpPolicy\",\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    "    n_steps=1024,\n",
    "    batch_size=128,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.0,\n",
    ")\n",
    "\n",
    "# 4) Env factory\n",
    "def make_env(seed=0, brake_profile=True, normalize_obs=True):\n",
    "    def _thunk():\n",
    "        return ACCEnv(brake_profile=brake_profile, normalize_obs=normalize_obs, seed=seed)\n",
    "    return _thunk\n",
    "\n",
    "# 5) Create vectorized normalized training env\n",
    "base_env = DummyVecEnv([make_env(seed=SEED, brake_profile=False, normalize_obs=True) for _ in range(N_ENVS)])\n",
    "train_env = VecNormalize(base_env, norm_obs=True, norm_reward=True, clip_obs=1.0)\n",
    "print(\"Train env created. obs_space:\", train_env.observation_space, \"act_space:\", train_env.action_space)\n",
    "\n",
    "# 6) Create model, set logger, train\n",
    "model = PPO(**PPO_PARAMS, env=train_env)\n",
    "model.set_logger(configure(LOGDIR, [\"stdout\", \"csv\", \"tensorboard\"]))\n",
    "print(f\"Starting training for {TOTAL_TIMESTEPS} timesteps ...\")\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 7) Save model + VecNormalize\n",
    "model_path = os.path.join(LOGDIR, \"ppo_acc\")\n",
    "vec_path = os.path.join(LOGDIR, \"vecnormalize.pkl\")\n",
    "model.save(model_path)        # writes ppo_acc.zip\n",
    "train_env.save(vec_path)      # writes vecnormalize.pkl\n",
    "print(\"Saved model ->\", model_path + \".zip\")\n",
    "print(\"Saved VecNormalize ->\", vec_path)\n",
    "\n",
    "# --------------------- Smoke-check (fixed) ---------------------\n",
    "\n",
    "# Robust wrappers for reset/step across gym/sb3 versions\n",
    "def reset_unwrap(env, **kwargs):\n",
    "    out = env.reset(**kwargs)\n",
    "    if isinstance(out, tuple) and len(out) == 2:\n",
    "        obs, _info = out\n",
    "        return obs\n",
    "    return out  # obs only\n",
    "\n",
    "def step_unwrap(env, action):\n",
    "    out = env.step(action)\n",
    "    if len(out) == 5:\n",
    "        obs, reward, terminated, truncated, info = out\n",
    "        return obs, reward, bool(terminated), bool(truncated), info\n",
    "    elif len(out) == 4:\n",
    "        obs, reward, done, info = out\n",
    "        return obs, reward, bool(done), False, info\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected env.step() return length: {len(out)}\")\n",
    "\n",
    "# Build an evaluation env that uses the saved normalization stats\n",
    "eval_base = DummyVecEnv([make_env(seed=123, brake_profile=True, normalize_obs=True)])\n",
    "eval_env = VecNormalize.load(vec_path, eval_base)\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# load saved model, bound to eval_env\n",
    "model_loaded = PPO.load(model_path + \".zip\", env=eval_env)\n",
    "print(\"Loaded model and VecNormalize for evaluation.\")\n",
    "\n",
    "def run_eval_episode(mdl, env, max_steps=1000):\n",
    "    obs = reset_unwrap(env)\n",
    "    total_r = 0.0\n",
    "    collided = False\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action, _ = mdl.predict(obs, deterministic=True)\n",
    "        obs, r, term, trunc, info = step_unwrap(env, action)\n",
    "        total_r += float(r[0]) if hasattr(r, \"__len__\") else float(r)\n",
    "        steps += 1\n",
    "        idict = info if isinstance(info, dict) else (info[0] if hasattr(info, \"__len__\") and len(info) else {})\n",
    "        if idict.get(\"collision\", False):\n",
    "            collided = True\n",
    "        if term or trunc or (steps >= max_steps):\n",
    "            break\n",
    "    return total_r, collided, steps\n",
    "\n",
    "print(\"Running 3 smoke-check episodes ...\")\n",
    "for i in range(3):\n",
    "    ret, coll, steps = run_eval_episode(model_loaded, eval_env)\n",
    "    print(f\"Episode {i}: return={ret:.3f}, collision={coll}, steps={steps}\")\n",
    "\n",
    "# Optional: quick plot of training CSV if available\n",
    "csv_path = os.path.join(LOGDIR, \"progress.csv\")\n",
    "if os.path.exists(csv_path):\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if 'rollout/ep_rew_mean' in df.columns:\n",
    "            plt.figure(figsize=(8,3))\n",
    "            plt.plot(df['rollout/ep_rew_mean'])\n",
    "            plt.title(\"rollout/ep_rew_mean during training\")\n",
    "            plt.xlabel(\"logging step\")\n",
    "            plt.ylabel(\"mean episode reward\")\n",
    "            plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nDone. You now have:\")\n",
    "print(\" -\", model_path + \".zip\")\n",
    "print(\" -\", vec_path)\n",
    "print(\"These artifacts will be loaded by your attacks notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b5316-3541-487d-8f7c-cf0616457865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Make model/env ready for the demo ===\n",
    "import os, sys, subprocess, shlex\n",
    "from IPython import get_ipython\n",
    "\n",
    "# 1) Ensure ACCEnv is defined (import .py; else convert .ipynb -> .py; else %run notebook)\n",
    "try:\n",
    "    from acc_env import ACCEnv  # if you already have acc_env.py alongside this notebook\n",
    "    print(\"Imported ACCEnv from acc_env.py\")\n",
    "except ModuleNotFoundError:\n",
    "    if os.path.exists(\"acc_env.ipynb\"):\n",
    "        print(\"Converting acc_env.ipynb -> acc_env.py ...\")\n",
    "        subprocess.run(shlex.split(\"jupyter nbconvert --to python acc_env.ipynb\"), check=True)\n",
    "        if os.getcwd() not in sys.path:\n",
    "            sys.path.append(os.getcwd())\n",
    "        from acc_env import ACCEnv\n",
    "        print(\"Imported ACCEnv from converted acc_env.py\")\n",
    "    else:\n",
    "        print(\"Running acc_env.ipynb directly...\")\n",
    "        get_ipython().run_line_magic(\"run\", \"./acc_env.ipynb\")\n",
    "        # ACCEnv should now be in globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4324d97-6013-4741-a4b6-aa4d1c615bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Try to load saved PPO + VecNormalize; otherwise quick-train a small model\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "LOGDIR = \"runs/ppo_baseline\"  # change if you saved elsewhere\n",
    "vec_path = os.path.join(LOGDIR, \"vecnormalize.pkl\")\n",
    "mdl_path = os.path.join(LOGDIR, \"ppo_acc.zip\")\n",
    "\n",
    "def make_env(seed=123, brake_profile=True, normalize_obs=True):\n",
    "    def _thunk():\n",
    "        return ACCEnv(brake_profile=brake_profile, normalize_obs=normalize_obs, seed=seed)\n",
    "    return _thunk\n",
    "\n",
    "model = None\n",
    "env = None\n",
    "\n",
    "if os.path.exists(vec_path) and os.path.exists(mdl_path):\n",
    "    print(f\"Loading saved model/env from {LOGDIR} ...\")\n",
    "    base_env = DummyVecEnv([make_env(seed=123, brake_profile=True, normalize_obs=True)])\n",
    "    env = VecNormalize.load(vec_path, base_env)\n",
    "    env.training = False\n",
    "    env.norm_reward = False\n",
    "    model = PPO.load(mdl_path, env=env)\n",
    "    print(\"Loaded saved PPO and VecNormalize.\")\n",
    "else:\n",
    "    print(\"Saved files not found; doing a quick in-memory train so the demo can run...\")\n",
    "    # quick train on a stationary-lead scenario (no braking) so it learns *something*\n",
    "    train_env = DummyVecEnv([make_env(seed=42, brake_profile=False, normalize_obs=True)])\n",
    "    train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_obs=1.0)\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", train_env, seed=42, verbose=0,\n",
    "        n_steps=512, batch_size=128, learning_rate=3e-4,\n",
    "        gamma=0.99, gae_lambda=0.95, clip_range=0.2, ent_coef=0.0\n",
    "    )\n",
    "    model.learn(total_timesteps=8_000)  # small, fast\n",
    "    # build an eval env (with braking enabled) sharing the same VecNormalize statistics\n",
    "    eval_base = DummyVecEnv([make_env(seed=123, brake_profile=True, normalize_obs=True)])\n",
    "    env = VecNormalize(eval_base, norm_obs=True, norm_reward=False, clip_obs=1.0)\n",
    "    # copy normalization stats from training env so obs scales match what policy expects\n",
    "    env.obs_rms = train_env.obs_rms\n",
    "    env.ret_rms = train_env.ret_rms\n",
    "    env.training = False\n",
    "    print(\"Quick train done; model/env are ready.\")\n",
    "\n",
    "print(\"\\n✅ model and env are ready in this kernel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b8f33-cef9-4c18-9de7-40c62c27542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6062c61-99c7-4ade-8468-b6bc37005906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_tensor(x: np.ndarray) -> torch.Tensor:\n",
    "    return torch.as_tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86d3cb-5223-438b-9676-75fd66bd08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackWrapper:\n",
    "    \"\"\"Base wrapper that perturbs observations before the agent acts.\"\"\"\n",
    "    def __init__(self, model: Any, epsilon: float = 0.01, device: str = \"cpu\") -> None:\n",
    "        self.model = model\n",
    "        self.eps = float(epsilon)\n",
    "        self.device = device\n",
    "\n",
    "    def perturb(self, obs: np.ndarray) -> np.ndarray:\n",
    "        return obs\n",
    "\n",
    "    def act(self, obs: np.ndarray):\n",
    "        # Compute adversarial observation (gradients enabled in perturb),\n",
    "        # then call model.predict without gradients.\n",
    "        obs_adv = self.perturb(obs)\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.model.predict(obs_adv, deterministic=True)\n",
    "        return action, obs_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d2f36-1497-45c5-a59a-89b458f7cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSMAttack(AttackWrapper):\n",
    "    \"\"\"FGSM with respect to policy mean action (pre-squash).\"\"\"\n",
    "    def perturb(self, obs: np.ndarray) -> np.ndarray:\n",
    "        # prepare policy for gradients\n",
    "        self.model.policy.set_training_mode(True)\n",
    "        self.model.policy.zero_grad(set_to_none=True)\n",
    "\n",
    "        obs_t = _to_tensor(obs)\n",
    "        single = False\n",
    "        if obs_t.ndim == 1:\n",
    "            obs_t = obs_t.unsqueeze(0)\n",
    "            single = True\n",
    "        obs_t = obs_t.to(self.device)\n",
    "        obs_t.requires_grad_(True)\n",
    "\n",
    "        # forward through policy internals to get mean action\n",
    "        features = self.model.policy.extract_features(obs_t)\n",
    "        latent_pi, _ = self.model.policy.mlp_extractor(features)\n",
    "        mean_actions = self.model.policy.action_net(latent_pi)  # [B, act_dim]\n",
    "\n",
    "        # simple scalar objective: increase squared mean action\n",
    "        obj = (mean_actions ** 2).sum()\n",
    "        obj.backward()\n",
    "\n",
    "        grad_sign = torch.sign(obs_t.grad)\n",
    "        adv = torch.clamp(obs_t + self.eps * grad_sign, -1.0, 1.0)\n",
    "        adv_np = adv.detach().cpu().numpy()\n",
    "        return adv_np[0] if single else adv_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcab2b-56d1-4114-8c78-b23132490b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OIAttack(AttackWrapper):\n",
    "    \"\"\"Optimism Induction Attack: increase the critic value V(s).\"\"\"\n",
    "    def perturb(self, obs: np.ndarray) -> np.ndarray:\n",
    "        self.model.policy.set_training_mode(True)\n",
    "        self.model.policy.zero_grad(set_to_none=True)\n",
    "\n",
    "        obs_t = _to_tensor(obs)\n",
    "        single = False\n",
    "        if obs_t.ndim == 1:\n",
    "            obs_t = obs_t.unsqueeze(0)\n",
    "            single = True\n",
    "        obs_t = obs_t.to(self.device)\n",
    "        obs_t.requires_grad_(True)\n",
    "\n",
    "        features = self.model.policy.extract_features(obs_t)\n",
    "        _, latent_vf = self.model.policy.mlp_extractor(features)\n",
    "        values = self.model.policy.value_net(latent_vf)  # [B,1]\n",
    "\n",
    "        obj = values.sum()\n",
    "        obj.backward()\n",
    "\n",
    "        grad_sign = torch.sign(obs_t.grad)\n",
    "        adv = torch.clamp(obs_t + self.eps * grad_sign, -1.0, 1.0)\n",
    "        adv_np = adv.detach().cpu().numpy()\n",
    "        return adv_np[0] if single else adv_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873b849-fdd5-4797-a289-102a76645aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attack_sanity(model, env, eps=0.01):\n",
    "    atk = FGSMAttack(model, epsilon=eps, device=\"cpu\")\n",
    "    obs = env.reset()[0]\n",
    "    adv = atk.perturb(obs)\n",
    "    print(\"FGSM sanity:\")\n",
    "    print(\" original obs:\", obs)\n",
    "    print(\" adv obs     :\", adv)\n",
    "    print(\" max |Δ|     :\", float(np.max(np.abs(np.array(adv) - np.array(obs)))))\n",
    "\n",
    "    atk2 = OIAttack(model, epsilon=eps, device=\"cpu\")\n",
    "    adv2 = atk2.perturb(obs)\n",
    "    print(\"\\nOIA sanity:\")\n",
    "    print(\" original obs:\", obs)\n",
    "    print(\" adv obs     :\", adv2)\n",
    "    print(\" max |Δ|     :\", float(np.max(np.abs(np.array(adv2) - np.array(obs)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
